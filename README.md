# DATA-PIPELINE-DEVELOPMENT

COMPANY: CODETECH IT SOLUTION

NAME: MAYANK SAGAR

INTERN ID: CT04DN926

DOMAIN: DATA SCIENCE

DURATION: 4 WEEKS

MENTOR: NEELA SANTOSH


As part of my internship at CodTech, I was assigned the task of developing a structured data pipeline to process an employee dataset for machine learning purposes. This task required applying fundamental data preprocessing steps using Python and Scikit-learn to prepare raw data for predictive modeling. Below is a detailed breakdown of the process, presented step by step.

Step 1: Load the Dataset
The first step in the pipeline was to load the raw dataset named sample_data.csv, which was assumed to be located in the working directory. Using the Pandas library, I loaded the data into a DataFrame. This CSV file contained several columns, including both input features and a target column labeled target. Pandas’ read_csv() function enabled me to quickly inspect and manipulate the data for further processing.

Step 2: Separate Features and Target Variable
After loading the data, I separated the independent variables (features) from the target variable. This was done by dropping the target column from the DataFrame to form X and storing the target column in y. This step is essential because most machine learning algorithms require inputs (X) and outputs (y) to be clearly defined.

Step 3: Identify Column Types
Next, I manually identified and categorized the columns into two types: numerical and categorical. For this dataset, age and salary were selected as numerical features, while department and city were categorized as categorical features. This classification allowed me to apply tailored preprocessing strategies to each type of data.

Step 4: Create a Numerical Pipeline
To handle the numerical columns, I built a preprocessing pipeline using Scikit-learn’s Pipeline class. The pipeline consisted of two main steps: first, I used SimpleImputer with a strategy of filling missing values using the mean of each column. Then, I applied StandardScaler to normalize the values, bringing them to a standard scale. This scaling process is important because many machine learning models are sensitive to the scale of input data.

Step 5: Create a Categorical Pipeline
For the categorical features, I created a separate pipeline. In this case, I used SimpleImputer to replace missing values with the most frequent value in each column. After handling missing data, I used OneHotEncoder to convert categorical variables into a numerical format suitable for machine learning. The encoder was configured to ignore any unknown categories encountered during testing to ensure robustness.

Step 6: Combine Pipelines Using ColumnTransformer
I combined the numerical and categorical pipelines using Scikit-learn’s ColumnTransformer. This step allowed me to apply different transformations to different sets of columns in one unified preprocessing step, resulting in clean, modular, and efficient code.

Step 7: Split the Dataset
Before applying the transformations, I split the dataset into training and testing sets using train_test_split. I used a 67:33 split ratio, keeping a significant portion for training while leaving enough data for validation. This ensured that the pipeline could be tested on unseen data to verify its generalization.

Step 8: Apply the Preprocessing
With the training and test sets ready, I fit the preprocessor on the training data using fit_transform() and applied the same transformation to the test data using transform(). This ensured that the same preprocessing logic was used across both datasets.

Step 9: Extract and Label Feature Names
After transformation, I extracted the feature names generated by the OneHotEncoder and combined them with the original numerical column names. This step provided a complete set of labeled columns for the transformed datasets, making them easier to interpret and use for modeling.

Step 10: Save Transformed Data
Finally, I saved the transformed training and testing feature sets as X_train_transformed.csv and X_test_transformed.csv, respectively. I also saved the corresponding target values in y_train.csv and y_test.csv. These files now serve as ready-to-use inputs for any machine learning model.

This comprehensive, step-by-step approach to building the data pipeline not only helped me understand the technical aspects of preprocessing but also emphasized the importance of modularity, reusability, and reproducibility in machine learning workflows. This task was a critical component of my internship, allowing me to apply theoretical knowledge in a practical setting and strengthen my skills in data engineering using Python.

